{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_lags(df, target_col='Close', n_lags=30):\n",
    "    \"\"\"\n",
    "    Adds lag features to the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - target_col: The column for which lags will be created.\n",
    "    - n_lags: Number of lag features to create.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with lag features added.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        df_copy[f'{target_col}_lag_{lag}'] = df_copy[target_col].shift(lag)\n",
    "    return df_copy.dropna()\n",
    "\n",
    "# Add technical indicators\n",
    "def add_technical_indicators(df):\n",
    "    # Moving averages\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['20d_std'] = df['Close'].rolling(window=20).std()\n",
    "    df['upper_band'] = df['SMA_20'] + (df['20d_std'] * 2)\n",
    "    df['lower_band'] = df['SMA_20'] - (df['20d_std'] * 2)\n",
    "    \n",
    "    # MACD\n",
    "    df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "    df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # Volatility\n",
    "    df['volatility'] = df['Close'].rolling(window=20).std() / df['Close'].rolling(window=20).mean()\n",
    "    \n",
    "    # Volume features\n",
    "    df['volume_change'] = df['Volume'].pct_change()\n",
    "    df['volume_ma'] = df['Volume'].rolling(window=10).mean()\n",
    "    df['volume_ratio'] = df['Volume'] / df['volume_ma']\n",
    "    \n",
    "    # Price to moving average ratios\n",
    "    for horizon in [2, 5, 60, 250]:\n",
    "        rolling_averages = df['Close'].rolling(window=horizon).mean()\n",
    "        df[f'Close_ratio_{horizon}d_MA'] = df['Close'] / rolling_averages\n",
    "        \n",
    "        # Trend features\n",
    "        df[f'Trend_{horizon}d_MA'] = df['GreenDay'].shift(1).rolling(window=horizon).sum()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_seasonal_features(df):\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    df['quarter'] = df.index.quarter\n",
    "    \n",
    "    # One-hot encode day of week and month\n",
    "    for day in range(5):  # 0-4 for weekdays\n",
    "        df[f'day_{day}'] = (df['day_of_week'] == day).astype(int)\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        df[f'month_{month}'] = (df['month'] == month).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Inicio funcion feature_engineering\n",
    "\n",
    "def feature_engineering(data):\n",
    "\n",
    "\n",
    "    # Horizontes\n",
    "\n",
    "    horizons = [2,5,60,250] # two days, week, month, year\n",
    "    new_predictors = []\n",
    "\n",
    "    for horizon in horizons:\n",
    "        rolling_averages = data['Close'].rolling(window=horizon).mean()\n",
    "\n",
    "        ratio_column = f'Close_ratio_{horizon}d MA'\n",
    "        data[ratio_column] = data['Close'] / rolling_averages # how far is the current price from the horizon day moving average\n",
    "\n",
    "        trend_column = f'Trend_{horizon}d MA'\n",
    "        data[trend_column] = data['GreenDay'].shift(1).rolling(window=horizon).sum() # on any given day, how many green days have there been in the past horizon days\n",
    "        new_predictors.extend([ratio_column, trend_column])\n",
    "\n",
    "    # Lagged features\n",
    "\n",
    "    data = add_lags(data, target_col='Close', n_lags=30)\n",
    "\n",
    "    # Technical indicators\n",
    "\n",
    "    data  = add_technical_indicators(data)\n",
    "\n",
    "    # Seasonal features\n",
    "\n",
    "    data = add_seasonal_features(data)\n",
    "\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    data = data.dropna()\n",
    "\n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_size=0.8):\n",
    "\n",
    "    '''\n",
    "    Split data into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame): Data to split.\n",
    "        train_size (float): Size of training set.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Training and testing sets for X and y.'''\n",
    "    \n",
    "    # Split the data into features and target\n",
    "    X = data.drop(columns=['Close'])\n",
    "    y = data['Close']\n",
    "    # Split into training and testing sets\n",
    "    train_size = int(len(data) * train_size)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "# Fin de funcion split_data\n",
    "\n",
    "# Comienzo de funcion scale_data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_data(X_train, X_test, y_train=None, y_test=None, feature_scaler=None, target_scaler=None):\n",
    "    \"\"\"\n",
    "    Scales the training and testing data using MinMaxScaler or a provided scaler.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training features (numpy array or DataFrame).\n",
    "    - X_test: Testing features (numpy array or DataFrame).\n",
    "    - y_train: (Optional) Training target values (numpy array or Series).\n",
    "    - y_test: (Optional) Testing target values (numpy array or Series).\n",
    "    - feature_scaler: (Optional) Predefined scaler for features. If None, a new MinMaxScaler is created.\n",
    "    - target_scaler: (Optional) Predefined scaler for target values. If None, a new MinMaxScaler is created.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_scaled: Scaled training features.\n",
    "    - X_test_scaled: Scaled testing features.\n",
    "    - y_train_scaled: (Optional) Scaled training target values.\n",
    "    - y_test_scaled: (Optional) Scaled testing target values.\n",
    "    - feature_scaler: Scaler used for features.\n",
    "    - target_scaler: Scaler used for target values (if applicable).\n",
    "    \"\"\"\n",
    "    # Initialize scalers if not provided\n",
    "    if feature_scaler is None:\n",
    "        feature_scaler = MinMaxScaler()\n",
    "    if y_train is not None and target_scaler is None:\n",
    "        target_scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale features\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "    # Scale target values if provided\n",
    "    if y_train is not None and y_test is not None:\n",
    "        y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "        y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "        return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, feature_scaler, target_scaler\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, feature_scaler, target_scaler\n",
    "\n",
    "\n",
    "\n",
    "def create_sequences(X, y, time_steps=10):\n",
    "    \"\"\"\n",
    "    Transforma los datos en secuencias adecuadas para LSTM\n",
    "    \n",
    "    Parameters:\n",
    "    X (array): Features\n",
    "    y (array): Target\n",
    "    time_steps (int): NÃºmero de pasos de tiempo en cada secuencia\n",
    "    \n",
    "    Returns:\n",
    "    X_seq, y_seq: Datos transformados en secuencias\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i:i + time_steps])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
