{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "class LSTMModel:\n",
    "    \"\"\"\n",
    "    A wrapper class for LSTM Regression model with enhanced functionality\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_shape=None, \n",
    "                 units=50, \n",
    "                 dropout_rate=0.2, \n",
    "                 learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model with configurable parameters\n",
    "\n",
    "        Parameters:\n",
    "        - input_shape: Shape of input data (time_steps, features)\n",
    "        - units: Number of LSTM units\n",
    "        - dropout_rate: Dropout rate for regularization\n",
    "        - learning_rate: Learning rate for Adam optimizer\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.best_params = None\n",
    "\n",
    "    def _create_sequences(self, X, y, time_steps=10):\n",
    "        \"\"\"\n",
    "        Transform data into sequences suitable for LSTM\n",
    "        \n",
    "        Parameters:\n",
    "        X (array): Features\n",
    "        y (array): Target\n",
    "        time_steps (int): Number of time steps in each sequence\n",
    "        \n",
    "        Returns:\n",
    "        X_seq, y_seq: Data transformed into sequences\n",
    "        \"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(len(X) - time_steps):\n",
    "            X_seq.append(X[i:i + time_steps])\n",
    "            y_seq.append(y[i + time_steps])\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the LSTM model architecture\n",
    "        \n",
    "        Returns:\n",
    "        Compiled Keras LSTM model\n",
    "        \"\"\"\n",
    "        if self.input_shape is None:\n",
    "            raise ValueError(\"Input shape must be set before building the model\")\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # First LSTM layer with return sequences for stacking\n",
    "        model.add(LSTM(units=self.units, \n",
    "                      return_sequences=True, \n",
    "                      input_shape=self.input_shape, \n",
    "                      recurrent_dropout=self.dropout_rate/2))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        model.add(LSTM(units=self.units//2, \n",
    "                      return_sequences=False, \n",
    "                      recurrent_dropout=self.dropout_rate/2))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), \n",
    "                     loss='mse', \n",
    "                     metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, time_steps=10, batch_size=32, epochs=100):\n",
    "        \"\"\"\n",
    "        Train the LSTM model\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training features\n",
    "        - y_train: Training target values\n",
    "        - X_val: Validation features (optional)\n",
    "        - y_val: Validation target values (optional)\n",
    "        - time_steps: Number of time steps for sequences\n",
    "        - batch_size: Training batch size\n",
    "        - epochs: Number of training epochs\n",
    "        \"\"\"\n",
    "        # Ensure input is numpy array\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train).reshape(-1, 1)\n",
    "\n",
    "        # Create sequences\n",
    "        X_train_seq, y_train_seq = self._create_sequences(X_train, y_train, time_steps)\n",
    "        \n",
    "        # Set input shape if not already set\n",
    "        if self.input_shape is None:\n",
    "            self.input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        # Prepare validation data\n",
    "        validation_data = None\n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val = np.array(X_val)\n",
    "            y_val = np.array(y_val).reshape(-1, 1)\n",
    "            X_val_seq, y_val_seq = self._create_sequences(X_val, y_val, time_steps)\n",
    "            validation_data = (X_val_seq, y_val_seq)\n",
    "        \n",
    "        # Define callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001),\n",
    "            ModelCheckpoint('models/best_lstm_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        ]\n",
    "        \n",
    "        # Train the model\n",
    "        self.history = self.model.fit(\n",
    "            X_train_seq, y_train_seq,\n",
    "            validation_data=validation_data,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test, time_steps=10):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model\n",
    "\n",
    "        Parameters:\n",
    "        - X_test: Test features\n",
    "        - time_steps: Number of time steps for sequences\n",
    "\n",
    "        Returns:\n",
    "        Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        X_test = np.array(X_test)\n",
    "        X_test_seq, _ = self._create_sequences(X_test, np.zeros(len(X_test)), time_steps)\n",
    "        return self.model.predict(X_test_seq)\n",
    "\n",
    "    def evaluate(self, X_test, y_test, time_steps=10):\n",
    "        \"\"\"\n",
    "        Evaluate model performance\n",
    "\n",
    "        Parameters:\n",
    "        - X_test: Test features\n",
    "        - y_test: True target values\n",
    "        - time_steps: Number of time steps for sequences\n",
    "\n",
    "        Returns:\n",
    "        Dictionary of performance metrics\n",
    "        \"\"\"\n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test).reshape(-1, 1)\n",
    "        \n",
    "        X_test_seq, y_test_seq = self._create_sequences(X_test, y_test, time_steps)\n",
    "        y_pred = self.predict(X_test, time_steps)\n",
    "        \n",
    "        return {\n",
    "            'mse': mean_squared_error(y_test_seq, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test_seq, y_pred)),\n",
    "            'mae': mean_absolute_error(y_test_seq, y_pred),\n",
    "            'r2': r2_score(y_test_seq, y_pred)\n",
    "        }\n",
    "\n",
    "    def optimize_hyperparameters(self, X_train, y_train, X_val, y_val, time_steps=10):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter optimization\n",
    "\n",
    "        Parameters:\n",
    "        - X_train, y_train: Training data\n",
    "        - X_val, y_val: Validation data\n",
    "        - time_steps: Number of time steps for sequences\n",
    "\n",
    "        Returns:\n",
    "        Best hyperparameters\n",
    "        \"\"\"\n",
    "        # Prepare sequences\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train).reshape(-1, 1)\n",
    "        X_val = np.array(X_val)\n",
    "        y_val = np.array(y_val).reshape(-1, 1)\n",
    "        \n",
    "        X_train_seq, y_train_seq = self._create_sequences(X_train, y_train, time_steps)\n",
    "        X_val_seq, y_val_seq = self._create_sequences(X_val, y_val, time_steps)\n",
    "        \n",
    "        # Define hyperparameters for optimization\n",
    "        hyperparams = {\n",
    "            'units': [32, 64, 128],\n",
    "            'learning_rate': [0.001, 0.0005],\n",
    "            'dropout_rate': [0.2, 0.3],\n",
    "            'batch_size': [32, 64]\n",
    "        }\n",
    "        \n",
    "        best_params = None\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        # Generate all combinations of hyperparameters\n",
    "        param_combinations = list(product(\n",
    "            hyperparams['units'],\n",
    "            hyperparams['learning_rate'],\n",
    "            hyperparams['dropout_rate'],\n",
    "            hyperparams['batch_size']\n",
    "        ))\n",
    "        \n",
    "        for params in param_combinations:\n",
    "            units, learning_rate, dropout_rate, batch_size = params\n",
    "            \n",
    "            # Temporarily set model parameters\n",
    "            self.units = units\n",
    "            self.learning_rate = learning_rate\n",
    "            self.dropout_rate = dropout_rate\n",
    "            self.input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "            \n",
    "            # Build and train model\n",
    "            self.model = self.build_model()\n",
    "            history = self.model.fit(\n",
    "                X_train_seq, y_train_seq,\n",
    "                validation_data=(X_val_seq, y_val_seq),\n",
    "                epochs=10,\n",
    "                batch_size=batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Get validation loss\n",
    "            val_loss = history.history['val_loss'][-1]\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_params = {\n",
    "                    'units': units,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'batch_size': batch_size\n",
    "                }\n",
    "        \n",
    "        # Set best parameters\n",
    "        self.best_params = best_params\n",
    "        self.units = best_params['units']\n",
    "        self.learning_rate = best_params['learning_rate']\n",
    "        self.dropout_rate = best_params['dropout_rate']\n",
    "        \n",
    "        return best_params\n",
    "\n",
    "    def save(self, filepath='models/lstm_model.h5'):\n",
    "        \"\"\"\n",
    "        Save the trained model\n",
    "\n",
    "        Parameters:\n",
    "        - filepath: Path to save the model\n",
    "        \"\"\"\n",
    "        if not os.path.exists('models'):\n",
    "            os.makedirs('models')\n",
    "        self.model.save(filepath)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"\n",
    "        Load a pre-trained model\n",
    "\n",
    "        Parameters:\n",
    "        - filepath: Path to the saved model\n",
    "\n",
    "        Returns:\n",
    "        Loaded model\n",
    "        \"\"\"\n",
    "        return tf.keras.models.load_model(filepath)\n",
    "\n",
    "# Utility function for backward compatibility\n",
    "def train_lstm_model(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, time_steps=20):\n",
    "    \"\"\"\n",
    "    Convenience function to train an LSTM model\n",
    "\n",
    "    Parameters:\n",
    "    - X_train_scaled, y_train_scaled: Scaled training data\n",
    "    - X_test_scaled, y_test_scaled: Scaled testing data\n",
    "    - time_steps: Number of time steps for sequences\n",
    "\n",
    "    Returns:\n",
    "    best_model, history, X_test_seq, y_test_seq, best_params\n",
    "    \"\"\"\n",
    "    # Initialize and train LSTM model\n",
    "    lstm_model = LSTMModel()\n",
    "    lstm_model.fit(\n",
    "        X_train_scaled, y_train_scaled, \n",
    "        X_test_scaled, y_test_scaled, \n",
    "        time_steps=time_steps\n",
    "    )\n",
    "    \n",
    "    # Prepare test sequences for return\n",
    "    X_test_scaled = np.array(X_test_scaled)\n",
    "    y_test_scaled = np.array(y_test_scaled).reshape(-1, 1)\n",
    "    X_test_seq, y_test_seq = lstm_model._create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "    \n",
    "    return (\n",
    "        lstm_model.model, \n",
    "        lstm_model.history, \n",
    "        X_test_seq, \n",
    "        y_test_seq, \n",
    "        lstm_model.best_params or {}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
