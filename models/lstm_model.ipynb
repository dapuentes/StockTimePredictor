{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "def create_sequences(X, y, time_steps=10):\n",
    "    \"\"\"\n",
    "    Transform data into sequences suitable for LSTM\n",
    "    \n",
    "    Parameters:\n",
    "    X (array): Features\n",
    "    y (array): Target\n",
    "    time_steps (int): Number of time steps in each sequence\n",
    "    \n",
    "    Returns:\n",
    "    X_seq, y_seq: Data transformed into sequences\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i:i + time_steps])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, units=50, dropout_rate=0.2, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build an LSTM model with the specified architecture\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of input data (time_steps, features)\n",
    "    units (int): Number of LSTM units\n",
    "    dropout_rate (float): Dropout rate for regularization\n",
    "    learning_rate (float): Learning rate for Adam optimizer\n",
    "    \n",
    "    Returns:\n",
    "    model: Compiled Keras LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with return sequences for stacking\n",
    "    model.add(LSTM(units=units, \n",
    "                  return_sequences=True, \n",
    "                  input_shape=input_shape, \n",
    "                  recurrent_dropout=dropout_rate/2))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(units=units//2, \n",
    "                  return_sequences=False, \n",
    "                  recurrent_dropout=dropout_rate/2))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                 loss='mse', \n",
    "                 metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_simplified_grid_search(X_train, y_train, X_val, y_val, hyperparams):\n",
    "    \"\"\"\n",
    "    Simplified grid search that uses validation data for faster evaluation\n",
    "    \n",
    "    Parameters:\n",
    "    X_train, y_train: Training data\n",
    "    X_val, y_val: Validation data\n",
    "    hyperparams: Dictionary of hyperparameters to search\n",
    "    \n",
    "    Returns:\n",
    "    best_params: Dictionary of best hyperparameters\n",
    "    \"\"\"\n",
    "    best_params = None\n",
    "    best_val_loss = float('inf')\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Generate all combinations of hyperparameters\n",
    "    param_combinations = list(product(\n",
    "        hyperparams['units'],\n",
    "        hyperparams['learning_rate'],\n",
    "        hyperparams['dropout_rate'],\n",
    "        hyperparams['batch_size']\n",
    "    ))\n",
    "    \n",
    "    total_combinations = len(param_combinations)\n",
    "    print(f\"Testing {total_combinations} hyperparameter combinations...\")\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "        units, learning_rate, dropout_rate, batch_size = params\n",
    "        print(f\"Combination {i+1}/{total_combinations}: units={units}, lr={learning_rate}, dropout={dropout_rate}, batch_size={batch_size}\")\n",
    "        \n",
    "        # Build model with current parameters\n",
    "        model = build_lstm_model(input_shape, units, dropout_rate, learning_rate)\n",
    "        \n",
    "        # Train for only a few epochs to save time\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=10,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get validation loss\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        print(f\"Validation loss: {val_loss:.6f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = {\n",
    "                'units': units,\n",
    "                'learning_rate': learning_rate,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'batch_size': batch_size\n",
    "            }\n",
    "            print(f\"New best parameters found!\")\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_lstm_model(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, time_steps=20):\n",
    "    \"\"\"\n",
    "    Train an LSTM model with hyperparameter optimization\n",
    "    \n",
    "    Parameters:\n",
    "    X_train_scaled, y_train_scaled: Scaled training data\n",
    "    X_test_scaled, y_test_scaled: Scaled testing data\n",
    "    time_steps (int): Number of time steps for sequences\n",
    "    \n",
    "    Returns:\n",
    "    best_model: Trained LSTM model\n",
    "    history: Training history\n",
    "    X_test_seq: Sequence test data for predictions\n",
    "    y_test_seq: Sequence test targets\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays if they're DataFrames/Series\n",
    "    if isinstance(X_train_scaled, pd.DataFrame):\n",
    "        X_train_scaled = X_train_scaled.values\n",
    "    if isinstance(X_test_scaled, pd.DataFrame):\n",
    "        X_test_scaled = X_test_scaled.values\n",
    "    if isinstance(y_train_scaled, pd.Series):\n",
    "        y_train_scaled = y_train_scaled.values.reshape(-1, 1)\n",
    "    if isinstance(y_test_scaled, pd.Series):\n",
    "        y_test_scaled = y_test_scaled.values.reshape(-1, 1)\n",
    "    elif isinstance(y_train_scaled, np.ndarray) and len(y_train_scaled.shape) == 1:\n",
    "        y_train_scaled = y_train_scaled.reshape(-1, 1)\n",
    "    elif isinstance(y_test_scaled, np.ndarray) and len(y_test_scaled.shape) == 1:\n",
    "        y_test_scaled = y_test_scaled.reshape(-1, 1)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "    \n",
    "    print(f\"Training sequence shape: {X_train_seq.shape}\")\n",
    "    print(f\"Testing sequence shape: {X_test_seq.shape}\")\n",
    "    \n",
    "    # Define hyperparameters for optimization\n",
    "    lstm_hyperparams = {\n",
    "        'units': [32, 64, 128],\n",
    "        'learning_rate': [0.001, 0.0005],\n",
    "        'dropout_rate': [0.2, 0.3],\n",
    "        'batch_size': [32, 64]\n",
    "    }\n",
    "    \n",
    "    # Find best hyperparameters (simplified version to save time)\n",
    "    best_params = perform_simplified_grid_search(\n",
    "        X_train_seq, y_train_seq, X_test_seq, y_test_seq, lstm_hyperparams\n",
    "    )\n",
    "    \n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    \n",
    "    # Create model with best parameters\n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    best_model = build_lstm_model(\n",
    "        input_shape, \n",
    "        units=best_params['units'], \n",
    "        dropout_rate=best_params['dropout_rate'], \n",
    "        learning_rate=best_params['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001),\n",
    "        ModelCheckpoint('models/best_lstm_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    history = best_model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=100,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    \n",
    "    # Save model\n",
    "    best_model.save('models/lstm_model.h5')\n",
    "    \n",
    "    return best_model, history, X_test_seq, y_test_seq, best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
